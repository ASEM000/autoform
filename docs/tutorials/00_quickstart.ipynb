{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e2a953",
   "metadata": {},
   "source": [
    "# ðŸš€ ![Fundamentals](https://img.shields.io/badge/Fundamentals-4CAF50) Quickstart\n",
    "\n",
    "\n",
    "{{ badge }}\n",
    "\n",
    "A fast, example-driven tour of Autoform. This notebook covers the core workflow: trace a function, transform the IR, and execute it.\n",
    "\n",
    "Topics covered:\n",
    "\n",
    "- Tracing functions into IR with `trace`\n",
    "- Executing IR with `ir.call()` / `ir.acall()`\n",
    "- Debugging with `checkpoint` + `collect`\n",
    "- Vectorization with `batch`\n",
    "- Semantic gradients with `pullback`\n",
    "- Concurrent execution with `sched`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ceb117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T09:35:24.855829Z",
     "iopub.status.busy": "2026-01-22T09:35:24.855608Z",
     "iopub.status.idle": "2026-01-22T09:35:26.034121Z",
     "shell.execute_reply": "2026-01-22T09:35:26.033526Z"
    }
   },
   "outputs": [],
   "source": [
    "!uv pip install --quiet \"git+https://github.com/ASEM000/autoform.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5997884a",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "Key concepts:\n",
    "- Autoform uses LiteLLM as backend\n",
    "- Any LiteLLM-supported provider works https://docs.litellm.ai/docs/providers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90a4159",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T09:35:26.036031Z",
     "iopub.status.busy": "2026-01-22T09:35:26.035904Z",
     "iopub.status.idle": "2026-01-22T09:35:26.771910Z",
     "shell.execute_reply": "2026-01-22T09:35:26.771449Z"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import autoform as af\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY: \")\n",
    "\n",
    "MODEL = \"openai/gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e0f31",
   "metadata": {},
   "source": [
    "## 1) Trace with `trace`\n",
    "\n",
    "Key concepts:\n",
    "- `trace(func)(examples)` captures a function as IR\n",
    "- IR is a data structure representing the computation\n",
    "- Example inputs define structure; values replaced at call time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3c9b1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T09:35:26.773113Z",
     "iopub.status.busy": "2026-01-22T09:35:26.773004Z",
     "iopub.status.idle": "2026-01-22T09:35:26.775915Z",
     "shell.execute_reply": "2026-01-22T09:35:26.775428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func(%0:IRVar[str]) -> (%1:IRVar[str]) {\n",
      "  (%1:IRVar[str]) = format(%0:IRVar[str], template='Hello, {}!', keys=())\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def greet(name: str) -> str:\n",
    "    return af.format(\"Hello, {}!\", name)\n",
    "\n",
    "\n",
    "# NOTE: \"...\" is a placeholder; the actual value is passed at call time\n",
    "ir_greet = af.trace(greet)(\"...\")\n",
    "print(ir_greet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ce2a3",
   "metadata": {},
   "source": [
    "## 2) Execute with `ir.call()`\n",
    "\n",
    "Key concepts:\n",
    "- `ir.call(inputs)` runs synchronously\n",
    "- `ir.acall(inputs)` runs asynchronously\n",
    "- Inputs replace placeholders from tracing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b93d8f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T09:35:26.776995Z",
     "iopub.status.busy": "2026-01-22T09:35:26.776936Z",
     "iopub.status.idle": "2026-01-22T09:35:26.778556Z",
     "shell.execute_reply": "2026-01-22T09:35:26.778231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Alice!\n"
     ]
    }
   ],
   "source": [
    "result = ir_greet.call(\"Alice\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f8e9c",
   "metadata": {},
   "source": [
    "## 3) Structured LLM calls with `struct_lm_call`\n",
    "\n",
    "Key concepts:\n",
    "- `struct_lm_call(messages, model=..., struct=...)` returns typed output\n",
    "- Schema enforced at LLM level, no parsing needed\n",
    "- `Struct` fields are differentiable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0e5c3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T09:35:26.779470Z",
     "iopub.status.busy": "2026-01-22T09:35:26.779408Z",
     "iopub.status.idle": "2026-01-22T09:35:28.436502Z",
     "shell.execute_reply": "2026-01-22T09:35:28.436048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value='4' confidence='high'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "class Answer(af.Struct):\n",
    "    value: str\n",
    "    confidence: Literal[\"low\", \"medium\", \"high\"]\n",
    "\n",
    "\n",
    "def ask(question: str) -> Answer:\n",
    "    msg = dict(role=\"user\", content=question)\n",
    "    return af.struct_lm_call([msg], model=MODEL, struct=Answer)\n",
    "\n",
    "\n",
    "ir_ask = af.trace(ask)(\"...\")\n",
    "print(ir_ask.call(\"What is 2+2?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint_md",
   "metadata": {},
   "source": [
    "## 4) Debug with `checkpoint` and `collect`\n",
    "\n",
    "Key concepts:\n",
    "- `checkpoint(value, key=...)` marks a value for capture\n",
    "- `collect(collection=...)` returns captured values\n",
    "- Non-invasive: original computation unchanged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "checkpoint_code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T09:35:28.438290Z",
     "iopub.status.busy": "2026-01-22T09:35:28.438182Z",
     "iopub.status.idle": "2026-01-22T09:35:28.441418Z",
     "shell.execute_reply": "2026-01-22T09:35:28.440918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Step 2: Step 1: input\n",
      "Captured: defaultdict(<class 'list'>, {'after_step1': ['Step 1: input']})\n"
     ]
    }
   ],
   "source": [
    "def pipeline(x: str) -> str:\n",
    "    step1 = af.format(\"Step 1: {}\", x)\n",
    "    step1 = af.checkpoint(step1, key=\"after_step1\", collection=\"debug\")\n",
    "    step2 = af.format(\"Step 2: {}\", step1)\n",
    "    return step2\n",
    "\n",
    "\n",
    "ir_pipe = af.trace(pipeline)(\"...\")\n",
    "\n",
    "# NOTE: collect is a context manager, not a function call\n",
    "with af.collect(collection=\"debug\") as collected:\n",
    "    result = ir_pipe.call(\"input\")\n",
    "\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Captured: {collected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch_md",
   "metadata": {},
   "source": [
    "## 5) Vectorize with `batch`\n",
    "\n",
    "Key concepts:\n",
    "- `batch(ir, in_axes=...)` creates vectorized IR\n",
    "- `True` for batched inputs, `False` for broadcast\n",
    "- Similar to `jax.vmap` for text operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "batch_code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T09:35:28.442741Z",
     "iopub.status.busy": "2026-01-22T09:35:28.442652Z",
     "iopub.status.idle": "2026-01-22T09:35:28.445822Z",
     "shell.execute_reply": "2026-01-22T09:35:28.445313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UPPERCASE] hello world', '[UPPERCASE] goodbye moon', '[UPPERCASE] good morning sun']\n"
     ]
    }
   ],
   "source": [
    "def rewrite(style: str, text: str) -> str:\n",
    "    return af.format(\"[{}] {}\", style, text)\n",
    "\n",
    "\n",
    "ir_rewrite = af.trace(rewrite)(\"...\", \"...\")\n",
    "\n",
    "# NOTE: style is broadcast (False), texts are batched (True)\n",
    "ir_batched = af.batch(ir_rewrite, in_axes=(False, True))\n",
    "\n",
    "texts = [\"hello world\", \"goodbye moon\", \"good morning sun\"]\n",
    "results = ir_batched.call((\"UPPERCASE\", texts))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pullback_md",
   "metadata": {},
   "source": [
    "## 6) Semantic gradients with `pullback`\n",
    "\n",
    "Key concepts:\n",
    "- `pullback(ir)` creates a gradient IR\n",
    "- Input: `((inputs), feedback)`, Output: `(output, grad_inputs)`\n",
    "- Foundation for prompt optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pullback_code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T09:35:28.447064Z",
     "iopub.status.busy": "2026-01-22T09:35:28.446977Z",
     "iopub.status.idle": "2026-01-22T09:35:31.473327Z",
     "shell.execute_reply": "2026-01-22T09:35:31.472210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Autoform is a library designed to facilitate the creation and management of composable transformations in large language model (LLM) programs. It provides a framework that allows developers to efficiently build and manipulate LLM-based applications by composing various program transformations. This can streamline the development process and enhance the capabilities of LLMs through modular and reusable components.\n",
      "Gradient: To address the feedback and improve the OUTPUT by including more detail about transformations, adjust the INPUT as follows:\n",
      "\n",
      "1. **Specify Types of Transformations**: Include information on the types of transformations Autoform supports (e.g., syntax transformations, semantic transformations, or optimization transformations).\n",
      "\n",
      "2. **Describe Key Features**: Highlight key features of Autoform such as modularity, reusability, and ease of integration with existing LLM systems.\n",
      "\n",
      "3. **Provide Use Cases**: Incorporate examples or use cases of how Autoform can be applied in real-world LLM applications to illustrate its practical benefits.\n",
      "\n",
      "4. **Expand on Benefits**: Elaborate on how Autoform enhances development efficiency, reduces complexity, and improves scalability and flexibility of LLM applications.\n",
      "\n",
      "Revised INPUT suggestion:\n",
      "\n",
      "\"Summarize: Autoform is a library for composable LLM program transformations, supporting various types such as syntax and optimization transformations. It features modularity, reusability, and integration capabilities with existing LLM systems. Autoform streamlines program development through enhanced efficiency, reduced complexity, and improved scalability, with use cases in optimizing semantic parsing and generating more efficient model pipelines.\"\n",
      "\n",
      "This revised INPUT provides a more comprehensive overview, addressing the feedback on providing more detail about transformations.\n"
     ]
    }
   ],
   "source": [
    "def summarize(text: str) -> str:\n",
    "    msg = dict(role=\"user\", content=af.format(\"Summarize: {}\", text))\n",
    "    return af.lm_call([msg], model=MODEL)\n",
    "\n",
    "\n",
    "ir_sum = af.trace(summarize)(\"...\")\n",
    "ir_pb = af.pullback(ir_sum)\n",
    "\n",
    "text = \"Autoform is a library for composable LLM program transformations.\"\n",
    "feedback = \"Too short. Include more detail about transformations.\"\n",
    "\n",
    "# NOTE: pullback returns (output, grad_tree) where grad_tree matches input structure\n",
    "output, grad_text = ir_pb.call((text, feedback))\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"Gradient: {grad_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sched_md",
   "metadata": {},
   "source": [
    "## 7) Parallelize with `sched`\n",
    "\n",
    "Key concepts:\n",
    "- `sched(ir)` rewrites IR for parallel execution\n",
    "- Use with `ir.acall()` for concurrency\n",
    "- Write sequential code, run concurrent\n",
    "\n",
    "Compare sequential vs scheduled execution time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sched_code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T09:35:31.475489Z",
     "iopub.status.busy": "2026-01-22T09:35:31.475341Z",
     "iopub.status.idle": "2026-01-22T09:35:37.538893Z",
     "shell.execute_reply": "2026-01-22T09:35:37.538089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: 4.58s\n",
      "Scheduled:  1.48s\n",
      "Speedup:    3.1x\n"
     ]
    }
   ],
   "source": [
    "def parallel_calls(doc: str) -> str:\n",
    "    doc = af.stop_gradient(doc)\n",
    "    # NOTE: These two calls are independent\n",
    "    msg1 = dict(role=\"user\", content=af.format(\"Summarize in 1 sentence: {}\", doc))\n",
    "    summary = af.lm_call([msg1], model=MODEL)\n",
    "    msg2 = dict(role=\"user\", content=af.format(\"List 2 topics: {}\", doc))\n",
    "    topics = af.lm_call([msg2], model=MODEL)\n",
    "    return af.format(\"Summary: {}\\nTopics: {}\", summary, topics)\n",
    "\n",
    "\n",
    "ir_calls = af.trace(parallel_calls)(\"...\")\n",
    "ir_scheduled = af.sched(ir_calls)\n",
    "\n",
    "doc = \"Machine learning models require training data and compute resources.\"\n",
    "\n",
    "# Sequential: calls run one after another\n",
    "t1 = time.time()\n",
    "result_seq = ir_calls.call(doc)\n",
    "t_seq = time.time() - t1\n",
    "\n",
    "# Scheduled: independent calls run concurrently\n",
    "t2 = time.time()\n",
    "result_sched = await ir_scheduled.acall(doc)\n",
    "t_sched = time.time() - t2\n",
    "\n",
    "print(f\"Sequential: {t_seq:.2f}s\")\n",
    "print(f\"Scheduled:  {t_sched:.2f}s\")\n",
    "print(f\"Speedup:    {t_seq / t_sched:.1f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "af",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
